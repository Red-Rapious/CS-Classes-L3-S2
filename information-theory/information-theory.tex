\documentclass{../cs-classes/cs-classes}

\title{Information theory and coding}
\author{Bartek Blaszczyszyn\\ Notes by Antoine Groudiev}

\renewcommand*{\P}{\mathbb{P}}
\newcommand*{\E}{\mathbb{E}}
\newcommand*{\X}{\mathcal{X}}

\begin{document}
\section*{Introduction}
This document is Antoine Groudiev's class notes while following the class \emph{Théorie de l'information et codage} (Information theory and coding) at the Computer Science Department of ENS Ulm. It is freely inspired by Bartek Blaszczyszyn's class notes. 

This class contains two main parts: information theory, and coding theory. Information theory gives mathematical basis to build a notion of \emph{information quantity}: given a text, how to "weight" the information contained in the sequence of characters? Some languages are more concise, but still provide the same quantity of information. Coding theory aims at finding the most concise way to represent information, with the smallest number of characters. Such theory -- \emph{source coding} -- has many applications (storage, \dots). Another branch of coding is \emph{canal coding} -- allowing "repetition" in a message to avoid the loss of information in a canal.

\section{Entropy and source coding}
We shall introduce \emph{Shannon’s entropy} of a probability distribution on a discrete space and study its basic properties. Our goal is to prove \emph{Shannon’s source coding theorem} formulated in 1948. It will allow us to interpret the entropy as a notion of the \emph{amount of information} "carried" by random variables of a given distribution.

\subsection{Shannon's entropy}
Let $\X$ be a finite or countable set, and $p := \{p(x) \: | \: x\in \X \}$ be a probability distribution on $\mathcal{X}$.

\begin{definition}[Shannon's entropy] We define (Shannon's) entropy $H(p)$ of $p$ to be:
\begin{equation}
    H(p) := -\sum_{x\in\X}p(x)\log p(x)
\end{equation}
with the convention that $0\log 0 = 0$, and $a\log 0 = -\infty$ for $a>0$.
We will later on discuss the base of the logarithm.
\end{definition}

\begin{remark}
    In this mathematical generalisation, $\X$ is the equivalent of a symbol alphabet, and $p$ represents the text. The nature of the elements of $\X$ is not important: the entropy, the average, \dots depend only on $\X$ and on the distribution $p$. Therefore, we can re-label the elements of $\X$.
\end{remark}

\begin{definition}[Entropy of a random variable]
    Let $X$ be a random variable on $\X$ with distribution $p$, that is $\P(X=x) = p(x)$, also denoted $X \sim p$. We define:
    \begin{equation}
        H(X) := H(p) = -\E(\log p(X))
    \end{equation}
\end{definition}

Observe that $0\leq H(p) \leq +\infty$, and that $H(p)=0$ if and only if $X$ is constant almost surely.

\begin{property}
    Entropy is invariant with respect to deterministic injective mapping $f: \X \to \mathcal{Y}$:
    \begin{equation*}
        H(X) = H(f(X))
    \end{equation*}
\end{property}

The entropy $H(p)$ can be interpreted as the \emph{amount of information} carried on average by one realization from the distribution $p$. Later in this chapter, we shall prove a result supporting this interpretation. 

\begin{definition}[Entropy units] The unit of the entropy depends on the \emph{base of the logarithm}:
    \begin{itemize}
        \item In binary basis, when $\log = \log_2$, we denote $H(p)=H_2(p)$, and its unit is the $[bit/symbol]$ (per realization of $X$).
        \item In arbitrary basis $b>0$, when $\log = \log_b$, we denote $H(p)=H_b(p)$, and its unit is the $[b-digit/symbol]$ (a $b$-digit is a digit which can take $b$ values).
        \item In basis $e$, when $\log = \ln$, we denote $H(p)=H_e(p)$, and its unit is the $[nat/symbol]$ (nat is the natural unit of information).
    \end{itemize}
    
    The conversion between units can be done by changing the base of the logarithm:
    \begin{equation*}
        H_b(p) = \frac{H_2(p)}{\log_2(b)}
    \end{equation*}
\end{definition}

\begin{example}[Bernoulli distribution]
    Let $\mathcal{X}=\{0, 1\}$, and $p$ the Bernoulli distribution such as
    \begin{equation*}
        \begin{cases}
            p(0) = p\\
            p(1) = 1 - p
        \end{cases}
    \end{equation*}
    Therefore, we have $H(p)=-p\log(p) - (1-p)\log(1-p)$. The Bernoulli distribution with the maximum entropy is:
    \begin{equation*}
        \max_{0\leq p\leq 1} H_2(p) = H_2(1/2) = 1 \: [bit/symbol]
    \end{equation*}
    % TODO: graph of entropy depending on p
\end{example}

\begin{example}[Uniform distribution]
    Let $\mathcal{X}$ be a finite set, and $p$ the uniform distribution, that is:
    \begin{equation*}
        \forall x\in \X, \: p(x) := \frac{1}{|\X|}
    \end{equation*}
    Therefore, we have $H(p) = \log(|X|)$.
\end{example}

\begin{example}[Geometric distribution]
    Let $\X = \N^\star$ and $p$ the geometric distribution of parameter $p>0$, that is:
    \begin{equation*}
        \forall n\in \N^\star, \: p(n) = p(1-p)^{n-1}
    \end{equation*}
    Recall that $\E[X] = \frac{1}{p}$ when $X$ follows a geometric law of parameter $p$.

    Therefore, we have:
    \begin{equation*}
        H(p) = \log\left(\frac{1-p}{p}\right) - \frac{1}{p}\log(1-p)
    \end{equation*}
\end{example}

\subsection{Gibbs' inequality}
\begin{theorem}[Gibbs' inequality]
    Let $p$ and $q$ be two probability distributions on $\X$. Then:
    \begin{equation}
        \label{eq:gibbs}
        H(p) = -\sum_{x\in\X} p(x)\log p(x)  \leq - \sum_{x\in\X} p(x)\log q(x)
    \end{equation}
    Moreover, if $H(p)<\infty$, then there is equality in \eqref{eq:gibbs} if and only if $p=q$.
\end{theorem}
The right-hand-side of \eqref{eq:gibbs} is called \emph{cross entropy} between $p$ and $q$.

\begin{proof}
    Let $x\sim p$. Gibbs' inequality is equivalent to:
    \begin{equation*}
        \E[\log p(X)] \geq \E[\log q(X)]
    \end{equation*}
    If $\E[\log q(X)] = -\infty$, the inequality is trivial. Otherwise, since we have $\E[\log q(X)] \leq 0$:
    \begin{equation*}
        \begin{aligned}
            \E[\log q(X)] - \E[\log p(X)] &= \E[\log q(X) - \log p(X)] \\
            &= \E\left[\log\left(\frac{q(X)}{p(X)}\right)\right]
        \end{aligned}
    \end{equation*}
    $\log$ being concave, by applying Jensen's inequality, we obtain:
    \begin{equation*}
        \begin{aligned}
            \E\left[\log\left(\frac{q(X)}{p(X)}\right)\right] &\leq \log\E\left[\frac{q(X)}{p(X)}\right] \\
            &= \log\sum_{x\in\X} \frac{q(x)}{p(X)}p(x)\\
            &= \log\sum_{x\in\X} q(x)\\
            &= \log 1 = 0
        \end{aligned}
    \end{equation*}
    The equality in Jensen's inequality holds if and only if $\frac{q(X)}{p(X)}$ is almost surely constant, that is $p=\lambda q$ almost surely; furthermore, we must have $\lambda=1$ since both $p$ and $q$ are distributions, hence $p=q$ almost surely. 
\end{proof}

\begin{corollary}[Uniform distribution maximizes entropy]
    Let $p$ be a probability distribution on some set $\X$ with $|\X|<\infty$. Then:
    \begin{equation*}
        0\leq H(p) \leq \log(|\X|)
    \end{equation*}
    and the equality holds if and only if $p$ is uniform on $\X$.
\end{corollary}

\begin{proof}
    Let $X \sim p$ and be $q$ the uniform distribution on $\X$. By Gibbs' inequality:
    \begin{equation*}
        H(p) \leq -\sum_{x\in\X}p(x)\log\left(\frac{1}{|X|}\right) = \log|X|
    \end{equation*}
    Notice that $\log|X|$ is the entropy of the uniform distribution $q$.
\end{proof}

\begin{corollary}[Geometric distribution maximizes entropy in the set of probability measures on $\N^\star$ having given expectation]
    Let $p$ be a probability distribution on $\X=\N^\star$ with mean $\mu=\sum_{n\geq 1} np(n) < \infty$. Then:
    \begin{equation*}
        H(p)\leq\mu\log(\mu)-(\mu-1)\log(\mu-1)
    \end{equation*}
    where the right-hand-side is the entropy of the geometric distribution with parameter $1/\mu$.
\end{corollary}

\begin{proof}
    Let $p$ be a probability distribution on $\X=\N^\star$ with mean $\mu<\infty$, and $q$ the geometric distribution of parameter $1/\mu$. According to Gibbs' inequality,
    \begin{equation*}
        \begin{aligned}
            H(p)&\leq-\sum_{n\geq 1}p(n)\log q(n) \\
            &=-\sum_{n\geq 1}p(n)\log\left(\frac{1}{\mu}\left(1-\frac{1}{\mu}\right)^{n-1}\right)\\
            &=\sum_{n\geq 1}p(n)\log \mu - \sum_{n\geq 1}(n-1) p(n)\log\left(1-\frac{1}{\mu}\right) \\
            &=\log\mu - \log\left(1-\frac{1}{\mu}\right)\left(\mu - 1\right)\\
            &=\log\mu - \left(\log(\mu-1)-\log\mu\right)\left(\mu - 1\right)\\
            &=\log\mu + \mu\log\mu - \mu\log(\mu-1) + \log(\mu-1) - \log\mu\\
            &=\mu\log\mu - (\mu-1)\log(\mu-1) = H(q)
        \end{aligned}
    \end{equation*}
\end{proof}

\subsection{Entropy of random vectors}
\begin{definition}[Entropy of random vectors]
    Let $X:=(X_1, \dots, X_n)$ be a random vector on $\X=\X_1\times \dots \times \X_n$, for some $n\geq 1$, with distribution
    \begin{equation*}
        p(x_1^n)=p(x_1, \dots, x_n) = \P(X_1=x_1, \dots, X_n=x_n)
    \end{equation*}
    The entropy of $X$ is defined as the entropy of its distribution:
    \begin{equation}
        H(X)=-\sum_{x\in \X}p(x)\log p(x) = -\E[\log p(X)]
    \end{equation}
\end{definition}

\begin{property}[Entropy of independent variables]
    Let $X:=(X_1, \dots, X_n)$ be a vector of \emph{idenpendent}, random variables. Then:
    \begin{equation}
        H(X)=\sum_{i=1}^n H(X_i)
    \end{equation}
\end{property}

\begin{proof}
    Let $p$ be the joint distribution of $X$. By independence, $p(x)=\prod_{i=1}^n p_i(x_i)$. Hence:
    \begin{equation*}
        \begin{aligned}
            H(X)&=-\E[\log p(X)]\\
            &=-\E\left[\log\prod_{i=1}^n p_i(X_i)\right]\\
            &=-\E\left[\sum_{i=1}^n\log p_i(X_i)\right]\\
            &=\sum_{i=1}^n-\E[\log p_i(X_i)]\\
            &=\sum_{i=1}^n H(X_i)
        \end{aligned}
    \end{equation*}
\end{proof}

\begin{property}[Independence maximizes entropy]
    Let $X:=(X_1, \dots, X_n)$ be a vector of (arbitrary) random variables for some $n\geq 1$. Then:
    \begin{equation}
        H(X)\leq \sum_{i=1}^n H(X_i)
    \end{equation}
    Moreover, the equality holds if and only if $X_1, \dots, X_n$ are independent.
\end{property}

\begin{proof}
    By induction. If $n=1$, the results holds. Let $X$ be an $n$-vector of random variables and $X_{n+1}$ another random variable. Denote $q(x, y)=p(x)p_{n+1}(y)$, where $X\sim p$ and $X_{n+1}\sim p_{n+1}$, and $(X_1, \dots, X_n, X_{n+1})\sim p'$. Since:
    \begin{equation*}
        \begin{aligned}
            H(X) + H(X_{n+1}) &= -\E[\log p(X)+\log p_{n+1}(X_{n+1})]\\
            &=-\E[\log q(X, X_{n+1})]\\
            &\geq -\E[\log p'(X, X_{n+1})] = H(X_1, \dots, X_n, X_{n+1})
        \end{aligned}
    \end{equation*}
    by Gibbs' inequality, the property is hereditary. Furthermore, there is equality in Gibbs' when $p'=q$, hence when $X_{n+1}$ is independent from $X$, i.e. when $X_1, \dots, X_n, X_{n+1}$ are independent.
\end{proof}

\subsection{Typical sequences of random variables}
\begin{definition}[Typical sequences]
    Let $\X$ be a set with $D:=|\X|<\infty$, and $X=(X_1, \dots, X_n)\in \X^n$ a vector of independent and identically distributed random variables. Let $p$ be the distribution of $X$ on $\X$, with $p(x)=\prod_{i=1}^n p(x_i)$. We denote $H_D:=H_D(p)=-\E[\log_Dp(X)]$, expressed in $D$-digits/symbol.

    For $\epsilon>0$, the following subset of realizations of $\X^n$
    \begin{equation}
        A_\epsilon^{(n)}:=\left\{x\in \X^n \: : \: \big| -\frac{1}{n}\sum_{i=1}^n \log_Dp(x_i)-H_D\big|\leq \varepsilon\right\}\subseteq \X^n
    \end{equation}
    is called the set of $\epsilon$-typical vectors in $\X^n$ with respect to $p$.

    Intuitively, the typical vectors are the vectors that probabilistically appear a lot, and that we need to represent faithfully.
\end{definition}

\begin{remark}
    \begin{equation*}
        \E\left[-\frac{1}{n}\sum_{i=1}^n \log_Dp(X_i)\right] = \E[-\log_Dp(X)]=H_D
    \end{equation*}
    and, by the Law of Large Numbers (LNN for short):
    \begin{equation*}
        \lim_{n\to+\infty}-\frac{1}{n}\sum_{i=1}^n \log_Dp(X_i) = \E[-\log_D p(X)] = H_D
    \end{equation*}
    We shall see that the probability distribution of $X$ concentrates on the set of typical sequences, and, dependending on the entropy $H_D$, the dimension of this set can be smaller than $n$ (the dimension of the whole space $\X^n$).
\end{remark}

\begin{property}[Typical sequences concentrate probability]
    \label{prop:typ-seq-concentrate}
    Let $X=(X_1, \dots, X_n)$ be a vector of i.i.d. random variables, with $X_i\sim p$ on $\X$, and $D:=|\X|<\infty$. We have:
    \begin{equation}
        \label{eq:lim-p-1}
        \lim_{n\to+\infty}\P(X\in A_\epsilon^{(n)})=1
    \end{equation}
    and
    \begin{equation}
        \label{eq:card-A}
        |A_\epsilon^{(n)}|\leq D^{n(H_D+\epsilon)}
    \end{equation}
    \begin{figure}
        \center
        \begin{tikzpicture}
            \draw [thick, fill=gray, fill opacity=0.2, text opacity=1] (0, 0) rectangle (5, 5) node [above=50pt, pos=0.5] {$\X^n$: all realizations};
            \draw [thick, color=blue, fill=blue, fill opacity=0.2, text opacity=1] (2.5, 2.5) circle[radius=1.5] node [color=blue] {$A_\epsilon^{(n)}$};
            \draw [thick, <-] (3.2, 2.5) -- (5.5, 3) node [right] {probable realizations};
            \node[draw] at (7.5, 4.5) {$|\X^n|=D^n$};
            \node[draw] at (7.5, 2.2) {$|A_\epsilon^{(n)}|\leq D^{n(H_D+\epsilon)}$};
        \end{tikzpicture}
        \caption{Representation of $A_\epsilon^{(n)}$}
    \end{figure}
\end{property}

\begin{proof}
    \eqref{eq:lim-p-1} follows from the LLN. For \eqref{eq:card-A}, observe that:
    \begin{equation*}
        \begin{aligned}
            x\in A_\epsilon^{(n)}\implies& -\sum_{i=1}^n \log_D p(x_i)\leq n(H_D+\epsilon)\\
            \iff&\log_D\left(\prod_{i=1}^n p(X_i)\right)\geq -n(H_D+\epsilon)\\
            \iff&\log_Dp(x)\geq -n(H_D+\epsilon)\\
            \iff&p(x)\geq D^{-n(H_D+\epsilon)}
        \end{aligned}
    \end{equation*}
    and since:
    \begin{equation*}
        \begin{aligned}
            1\geq \P(X\in A_\epsilon^{(n)})&=\sum_{x\in A_\epsilon}p(x)\\
            &\geq |A_\epsilon^{(n)}|D^{-n(H_D+\epsilon)}
        \end{aligned}
    \end{equation*}
    which completes the proof.
\end{proof}

\begin{property}[$A_\epsilon^{(n)}$ is the smallest set concentrating probability]
    \label{prop:a-eps-smallest}
    Under the assumptions of Property \ref{prop:typ-seq-concentrate}, let $B\subseteq \X^n$ and $R>0$ such that
    \begin{equation*}
        \lim_{n\to+\infty}\P(X\in B)=1
    \end{equation*}
    and
    \begin{equation*}
        |B|\leq D^{nR}
    \end{equation*}
    Then $R\geq H_D$, that is that $A_\epsilon^{(n)}$ is the smallest set concentrating probability.
\end{property}

\begin{proof}
    Let $\epsilon>0$, and assume $D>1$, otherwise the result is trivial. Observe that:
    \begin{equation*}
        \begin{aligned}
            x\in A_\epsilon^{(n)}\implies&-\sum_{i=1}^n \log_D p(X_i)\geq n(H_D-\epsilon)\\
            \iff& p(x)\leq D^{-n(H_D-\epsilon)}
        \end{aligned}
    \end{equation*}
    Therefore,
    \begin{equation*}
        \begin{aligned}
            \P(X\in A_\epsilon^{(n)}\cap B)&\leq |B|D^{-n(H_D\epsilon)}\\
            &\leq D^{-n(H_D-R-\epsilon)}
        \end{aligned}
    \end{equation*}
    Since $D>1$ and $\lim_{n\to+\infty}\P(X\in A_\epsilon^{(n)}\cap B)=1$, we must have $H_D-R-\epsilon\leq 0$, meaning that $H_D-\epsilon\leq R$. We complete the proof by letting $\epsilon\to 0$. 
\end{proof}

\subsection{Entropy and source coding rate -- Shannon's first theorem}
\begin{definition}[Encoding and decoding]
    Let $X^n=(X_1, \dots, X_n)$ be a vector of i.i.d. random variables, with $X_i\sim p$ on $\X$. We call $X_i$ \emph{source symbols}, and the vector $X^n$ of $n$ source symbols is a \emph{source message} (or \emph{source word}, or \emph{block of source symbols}). 

    Our goal is to \emph{encode} the source message $X^n$ consisting of $n$ symbols using some (hopefully smaller) number of symbols in $\X$. That is, to represent $X^n$ via a function $Y^m=c^{(n)}(X^n)\in \X^m$ for some $m\leq n$, in such a way that one can recover $X^n$ from $Y^m$ via a \emph{decoding function} $d^{(n)}$ at least with high probability.
\end{definition}

\begin{definition}[Compression rate]
    The following ratio $R$ is called \emph{(sources) compression rate}:
    \begin{equation}
        R:=\frac{m}{n}
    \end{equation}
\end{definition}

\begin{definition}[Error probability]
    We define the \emph{error probability} $P_e^{(n)}$ to be:
    \begin{equation}
        P_e^{(n)}:=\P\left(d^{(n)}\left(c^{n}(X^n)\right)\neq X^n\right)
    \end{equation}
    which is nothing but the probability that the decoded message is different from the source message.
\end{definition}

\begin{theorem}[Source coding theorem -- Shannon's first theorem (1948)]
    \label{th:shannon-first}
    Let $X^n\in \X^n$ be a vector of i.i.d. random variables, with $X_i\sim p$ on $\X$. Denote $D:=|\X|$ with $1<D<\infty$. Then:
    \begin{equation}
        \label{eq:shannon-ft-1}
        \forall R>H_D(p), \quad \begin{cases}
            \exists c^{(n)}:\X^n\to \X^{\lceil nR\rceil} \\
            \exists d^{(n)}:\X^{\lceil nR\rceil}\to \X^n
        \end{cases}\: \textnormal{such that}\: \lim_{n\to+\infty}P_e(n)=0
    \end{equation}
    Furthermore,
    \begin{equation}
        \label{eq:shannon-ft-2}
        \forall R<H_D(p), \quad \begin{cases}
            \forall c^{(n)}:\X^n\to \X^{\lceil nR\rceil} \\
            \forall d^{(n)}:\X^{\lceil nR\rceil}\to \X^n
        \end{cases}\: \textnormal{we have}\: \lim_{n\to+\infty}P_e(n)>0
    \end{equation}
\end{theorem}

\begin{proof}
    Let's start by proving \eqref{eq:shannon-ft-1}. Let $R>H(p)$ and consider $0<\epsilon<R-H_D(p)$. For $n\geq1$, let $A_\epsilon^{(n)}$ be the set of $\epsilon$-typical sequences for the distribution $p$, and let $f^{(n)}$ be an injection of $A_\epsilon^{(n)}$ into $\X^{\lceil nR\rceil}$. Notice that such an injection exists by \eqref{eq:card-A}.

    Let $x_\star\in X^{\lceil nR\rceil}\setminus f^{(n)}(A_\epsilon^{(n)})$ arbitrary. Such an $x_\star$ exists since the inequality is strict in \eqref{eq:card-A} (we have $H_D+\epsilon<R$). We define the following coding function:
    \begin{equation*}
        c^{(n)}:=x\to \begin{cases*}
            f^{(n)}(x) & for $x\in A_\epsilon^{(n)}$\\
            x_\star & otherwise
        \end{cases*}
    \end{equation*}
    As a decoding function, consider the inverse of $f^{(n)}$ on its image $f^{(n)}(A_\epsilon^{(n)})$, completed arbitrarily on the whole domain $\X^{\lceil nD\rceil}$, that is:
    \begin{equation*}
        d^{(n)}:=x\to\begin{cases*}
            \left[f^{(n)}\right]^{-1}(x) & if $x\in f^{(n)}(A_\epsilon^{(n)})$ \\
            x_0 & otherwise
        \end{cases*}
    \end{equation*}
    for some arbitrary $x_0\in \X^n$. Finally, note that:
    \begin{equation*}
        P_e^{(n)}\leq \P(X\notin A_\epsilon^{(n)})
    \end{equation*}
    Therefore, \eqref{eq:shannon-ft-1} follows from \eqref{eq:lim-p-1}.

    The second statement, \eqref{eq:shannon-ft-2}, follows from Property \ref{prop:a-eps-smallest} considering the set:
    \begin{equation*}
        B:=\{x \:|\: d^{(n)}\left(c^{n}(x)\right) = x\}
    \end{equation*}
\end{proof}

\begin{remark}[Achievable compression rates]
    The source coding theorem -- Theorem \ref{th:shannon-first} -- says that independent $D$-symbols emitted by a source with distribution $p$ can be encoded asymptotically without errors using $H_D(p)$ encoding symbols per sources symbol. Note that
    \begin{equation*}
        H_D(p)\leq H_D(u)=\log_D(D)=1
    \end{equation*}
    where $u$ is the uniform distribution. The zero-error probability is approached asymptotically when increasing the length of the encoding blocks of source symbols.
\end{remark}

\begin{remark}[Source coding rates]
    In general, one may use different sets of coding symbols $\mathcal{Y}$, having arbitrary number $b:=|\mathcal{Y}|>1$ of elements (set of $b$-digits) together with some coding and decoding functions:
    \begin{equation*}
        \begin{cases}
            c^{(n)}:\X^n\mapsto \mathcal{Y}^{n''}\\
            d^{(n)}:\mathcal{Y}^{n''}\mapsto \X^n
        \end{cases}
    \end{equation*}
    In this more general scheme, the ratio:
    \begin{equation}
        R_s:=\frac{\textnormal{number of $b$-digits used to encode one source message}}{\textnormal{number of source symbols in one source message}} = \frac{n''}{n}
    \end{equation}
    is called \emph{(source) coding rate}. It is expressed in $b$-digits/(source) symbol. Considering a bijective mapping $\X^{n'}\mapsto \mathcal{Y}^{n''}$ with $n'$, $n''$ such that $D^{n'}=b^{n''}$ in conjunction with Shannon's first theorem, it is straightforward to see that
    \begin{equation*}
        H_D(p)\log_bD = H_b(p) \: \textnormal{[$b$-digit/(source) symbol]}
    \end{equation*}
    is the \emph{infimum of coding rates over asymptotically error-free source coding}.
    \begin{figure}[!ht]
        \center
        \begin{tikzpicture}
            \draw[->, very thick] (0, 0) -- (10, 0) node [below] {$R_s$ [bit/symbol]};
            \draw[thick] (5, 0.3) -- (5, -0.3) node [below] {$H_2(p)$};
            \fill (0, 0) circle[radius=0.07] node [below] {$0$};

            \fill [pattern=north east lines, pattern color=red] (0, 0) rectangle (5, 0.5) node [above=5pt, pos=0.5, color=red, text width=4cm, align=center] {no error-free source coding};
            \fill [pattern=north east lines, pattern color=green] (5, 0) rectangle (9.9, 0.5) node [above=5pt, pos=0.5, color=green, text width=5cm, align=center] {$\exists$ asymptotically error-free source coding};

        \end{tikzpicture}
        \caption{Source coding "phase transition"}
    \end{figure}
\end{remark}

\end{document}