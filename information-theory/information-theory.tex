\documentclass{../cs-classes/cs-classes}

\title{Information theory and coding}
\author{Bartek Blaszczyszyn\\ Notes by Antoine Groudiev}

\renewcommand*{\P}{\mathbb{P}}
\newcommand*{\E}{\mathbb{E}}
\newcommand*{\X}{\mathcal{X}}

\begin{document}
\section*{Introduction}
This document is Antoine Groudiev's class notes while following the class \emph{Théorie de l'information et codage} (Information theory and coding) at the Computer Science Department of ENS Ulm. It is freely inspired by Bartek Blaszczyszyn's class notes. 

\section{Entropy and source coding}
We shall introduce \emph{Shannon’s entropy} of a probability distribution on a discrete space and study its basic properties. Our goal is to prove \emph{Shannon’s source coding theorem} formulated in 1948. It will allow us to interpret the entropy as a notion of the \emph{amount of information} "carried" by random variables of a given distribution.

\subsection{Shannon's entropy}
Let $\X$ be a finite or countable set, and $p := \{p(x) \: | \: x\in \X \}$ be a probability distribution on $\mathcal{X}$.

\begin{definition}[Shannon's entropy] We define (Shannon's) entropy $H(p)$ of $p$ to be:
\begin{equation}
    H(p) := -\sum_{x\in\X}p(x)\log p(x)
\end{equation}
with the convention that $0\log 0 = 0$, and $a\log 0 = -\infty$ for $a>0$.
We will later on discuss the base of the logarithm.
\end{definition}

\begin{definition}[Entropy of a random variable]
    Let $X$ be a random variable on $\X$ with distribution $p$, that is $\P(X=x) = p(x)$, also denoted $X \sim p$. We define:
    \begin{equation}
        H(X) := H(p) = -\E(\log p(X))
    \end{equation}
\end{definition}

Observe that $0\leq H(p) \leq +\infty$, and that $H(p)=0$ if and only if $X$ is constant almost surely.

\begin{property}
    Entropy is invariant with respect to deterministic injective mapping $f: \X \to \mathcal{Y}$:
    \begin{equation*}
        H(X) = H(f(X))
    \end{equation*}
\end{property}

The entropy $H(p)$ can be interpreted as the \emph{amount of information} carried on average by one realization from the distribution $p$. Later in this chapter, we shall prove a result supporting this interpretation. 

\begin{definition}[Entropy units] The unit of the entropy depends on the \emph{base of the logarithm}:
    \begin{itemize}
        \item In binary basis, when $\log = \log_2$, we denote $H(p)=H_2(p)$, and its unit is the $[bit/symbol]$ (per realization of $X$).
        \item In arbitrary basis $b>0$, when $\log = \log_b$, we denote $H(p)=H_b(p)$, and its unit is the $[b-digit/symbol]$ (a $b$-digit is a digit which can take $b$ values).
        \item In basis $e$, when $\log = \ln$, we denote $H(p)=H_e(p)$, and its unit is the $[nat/symbol]$ (nat is the natural unit of information).
    \end{itemize}
    
    The conversion between units can be done by changing the base of the logarithm:
    \begin{equation*}
        H_b(p) = \frac{H_2(p)}{\log_2(b)}
    \end{equation*}
\end{definition}

\begin{example}[Bernoulli distribution]
    Let $\mathcal{X}=\{0, 1\}$, and $p$ the Bernoulli distribution such as
    \begin{equation*}
        \begin{cases}
            p(0) = p\\
            p(1) = 1 - p
        \end{cases}
    \end{equation*}
    Therefore, we have $H(p)=-p\log(p) - (1-p)\log(1-p)$. The Bernoulli distribution with the maximum entropy is:
    \begin{equation*}
        \max_{0\leq p\leq 1} H_2(p) = H_2(1/2) = 1 \: [bit/symbol]
    \end{equation*}
\end{example}

\begin{example}[Uniform distribution]
    Let $\mathcal{X}$ be a finite set, and $p$ the uniform distribution, that is:
    \begin{equation*}
        \forall x\in \X, \: p(x) := \frac{1}{|\X|}
    \end{equation*}
    Therefore, we have $H(p) = \log(|X|)$.
\end{example}

\begin{example}[Geometric distribution]
    Let $\X = \N^\star$ and $p$ the geometric distribution of parameter $p>0$, that is:
    \begin{equation*}
        \forall n\in \N^\star, \: p(n) = p(1-p)^{n-1}
    \end{equation*}
    Recall that $\E[X] = \frac{1}{p}$ when $X$ follows a geometric law of parameter $p$.

    Therefore, we have:
    \begin{equation*}
        H(p) = \log\left(\frac{1-p}{p}\right) - \frac{1}{p}\log(1-p)
    \end{equation*}
\end{example}

\subsection{Gibbs' inequality}
\begin{theorem}[Gibbs' inequality]
    Let $p$ and $q$ be two probability distributions on $\X$. Then:
    \begin{equation}
        \label{eq:gibbs}
        H(p) = -\sum_{x\in\X} p(x)\log p(x)  \leq - \sum_{x\in\X} p(x)\log q(x)
    \end{equation}
    Moreover, if $H(p)<\infty$, then there is equality in \eqref{eq:gibbs} if and only if $p=q$.
\end{theorem}
The right-hand-side of \eqref{eq:gibbs} is called \emph{cross entropy} between $p$ and $q$.

\begin{proof}
    Let $x\sim p$. Gibbs' inequality is equivalent to:
    \begin{equation*}
        \E[\log p(X)] \geq \E[\log q(X)]
    \end{equation*}
    If $\E[\log q(X)] = -\infty$, the inequality is trivial. Otherwise, since we have $\E[\log q(X)] \leq 0$:
    \begin{equation*}
        \begin{aligned}
            \E[\log q(X)] - \E[\log p(X)] &= \E[\log q(X) - \log p(X)] \\
            &= \E\left[\log\left(\frac{q(X)}{p(X)}\right)\right]
        \end{aligned}
    \end{equation*}
    $\log$ being concave, by applying Jensen's inequality, we obtain:
    \begin{equation*}
        \begin{aligned}
            \E\left[\log\left(\frac{q(X)}{p(X)}\right)\right] &\leq \log\E\left[\frac{q(X)}{p(X)}\right] \\
            &= \log\sum_{x\in\X} \frac{q(x)}{p(X)}p(x)\\
            &= \log\sum_{x\in\X} q(x)\\
            &= \log 1 = 0
        \end{aligned}
    \end{equation*}
    The equality in Jensen's inequality holds if and only if $\frac{q(X)}{p(X)}$ is almost surely constant, that is $p=\lambda q$ almost surely; furthermore, we must have $\lambda=1$ since both $p$ and $q$ are distributions, hence $p=q$ almost surely. 
\end{proof}

\begin{corollary}[Uniform distribution maximizes entropy]
    Let $p$ be a probability distribution on some set $\X$ with $|\X|<\infty$. Then:
    \begin{equation*}
        0\leq H(p) \leq \log(|\X|)
    \end{equation*}
    and the equality holds if and only if $p$ is uniform on $\X$.
\end{corollary}

\begin{proof}
    Let $X \sim p$ and be $q$ the uniform distribution on $\X$. By Gibbs' inequality:
    \begin{equation*}
        H(p) \leq -\sum_{x\in\X}p(x)\log\left(\frac{1}{|X|}\right) = \log|X|
    \end{equation*}
    Notice that $\log|X|$ is the entropy of the uniform distribution $q$.
\end{proof}

\begin{corollary}[Geometric distribution maximizes entropy in the set of probability measures on $\N^\star$ having given expectation]
    Let $p$ be a probability distribution on $\X=\N^\star$ with mean $\mu=\sum_{n\geq 1} np(n) < \infty$. Then:
    \begin{equation*}
        H(p)\leq(\mu+1)\log(\mu+1)-\mu\log\mu
    \end{equation*}
    where the right-hand-side is the entropy of the geometric distribution with parameter $1/\mu$.
\end{corollary}

\begin{proof}
    Let $p$ be a probability distribution on $\X=\N^\star$ with mean $\mu<\infty$, and $q$ the geometric distribution of parameter $1/\mu$. According to Gibbs' inequality,
    \begin{equation*}
        \begin{aligned}
            H(p)&\leq-\sum_{n\geq 1}p(n)\log q(n) \\
            &=-\sum_{n\geq 1}p(n)\log\left(\frac{1}{\mu}\left(1-\frac{1}{\mu}\right)^{n-1}\right)\\
            &=\sum_{n\geq 1}p(n)\log \mu - \sum_{n\geq 1}(n-1) p(n)\log\left(1-\frac{1}{\mu}\right) \\
            &=\log\mu - \log\left(1-\frac{1}{\mu}\right)\left(\mu - 1\right)\\
            &=\log\mu - \left(\log(\mu-1)-\log\mu\right)\left(\mu - 1\right)\\
            &=\log\mu + \mu\log\mu - \mu\log(\mu-1) + \log(\mu-1) - \log\mu\\
            &=\mu\log\mu - (\mu-1)\log(\mu-1) = H(q)
        \end{aligned}
    \end{equation*}
\end{proof}

\end{document}