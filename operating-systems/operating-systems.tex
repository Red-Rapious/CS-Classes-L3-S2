\documentclass[toc]{../cs-classes/cs-classes}

\title{Operating Systems}
\author{Timothy Bourke\\ Notes by Antoine Groudiev}

\newcommand*{\option}{\texttt{ option}}
\newcommand*{\none}{\texttt{None}}
\newcommand*{\some}{\texttt{Some }}
\newcommand*{\vmap}{\texttt{VMAP}}

\begin{document}
\begin{abstract}    
    This document is Antoine Groudiev's class notes while following the class \emph{Syst√®mes d'exploitation} (Operating Systems) at the Computer Science Department of ENS Ulm. It is freely inspired by Timothy Bourke's classes, and especially its slides.
\end{abstract}

\section*{Introduction}
An operating system is made of three main parts: a \emph{kernel}, \emph{libraries}, and \emph{applications}. It is useful for sharing memory, computation time, processors, and devices (keyboards, disks, graphics cards, \dots). It constitues an abstraction layer used as a base for building bigger systems. For example, the OS allows hardware independence for applications, and provides common services (such as file systems with access control) and allows concurrency and communication with protection and access control.

The kernel contains lots of data structures and functions used for bookkeeping, as well as abstract modules such as interfaces, data structures, and functions for services. It allows a low-level control of hardware, and manages the concurrency and the events.

\section{Virtual Memory}
\subsection{Introduction}
We will start by studying the concept of \emph{virtual memory}. An OS must share finite ressources among multiple users and applications. It provides an abstraction for building such applications, which do not need to worry about the complexity of memory management with other applications. Indeed, physical memory must be shared: but what happens if it runs out, or if one process tries to read or write the memory of another? This is where virtual memory comes in handy: it allows to run each process in a virtual address space, and selectively share memory between processes for them to communicate. Furthermore, it allows processes to use faster physical memory as a cache for files on slower disks.

Virtual memory is an abstraction provided by a sophisticated and elegant mix of hardware and software. Hardware capabilities include exceptions (synchronous interrupts), address translation (which we will study in this section), main memory and caching of files on disks. Software-wise, we will study the kernel memory system. Hardware is needed to intervene at the lowest-level -- each individual \texttt{mov} instruction -- and for speed. Software is needed to implement sophisticated, flexible algorithms and for pervasive integration within a kernel.

For application programmers, virtual memory is largely invisible. Only very few programmers ever have to deal directly with this low-level hardware and software. Nevertheless, we will study it because it pervades all levels of a computer system; understanding how it works gives a deeper understanding of how the system works. It also provides powerful capabilities that can be exploited in applications. OS programmers cannot avoid knowing about virtual memory.

\subsection{Physical and virtual addressing}
Physical addressing is used in "simple" systems like embedded microcontrollers, in devices like cars, elevators, digital picture frames, \dots

Conversely, \emph{virtual addressing} is used in all modern servers, desktops, laptops and high-end mobile phones. It uses an MMU (Memory Management Unit), which translated the virtual address (VA) to a physical address (PA).

\subsection{Virtual memory address translation}
A \emph{virtual address space} is a set $V$ of $|V|=:N=2^n$ virtual addresses used within programs. These addresses are mapped to a \emph{physical address space}, which is a set $P$ of $|P|=:M=2^m$ physical addresses of DRAM.

The address translation can be formalised by a function $\vmap:V\to P \option$. For some virtual address $a$, we have
\begin{equation*}
    \vmap(a) = \begin{cases*}
        \some a' & if data at virtual address $a$ is at physical address a' in $P$\\
        \none & if data at virtual address $a$ is not in physical memory
    \end{cases*}
\end{equation*}
We can represent this simple model of the function \vmap by a data structure accessed by hardware (the MMU), and manipulated by software (the OS). 

We could think of individually mapping each byte or word, but it is too complicated in terms of software, and too expensive in terms of hardware and data structure. Thus, the address spaces are divided into numbered \emph{pages}, which are blocks of a specific size, e.g. 4 KiB. They are always aligned on \emph{page size}, i.e. they cannot overlap. The mapping is therefore represented by a \emph{page table}. In first approximation, it is a list of entries, one for each virtual page, specifing either \none (when there is no mapping), or $\some p$ where $p$ is a physical page number.

\subsection{VM as a tool for memory management}
The key idea is that each process has its own virtual address space. It can view memory as a simple linear array of bytes. Mapping function scatters addresses through physical memory. Well-chosen mappings simplify memory allocation and management. Each virtual page can be mapped to any physical page. A virtual page can be stored in different physical pages at different times. This allows to share code and data among processes, by mapping virtual pages to the same physical page.

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.6]{images/address-translation.png}
    \caption{Address translation}
\end{figure}

\subsection{Address translation}
The following diagram shows the translation of addresses with a page table:
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{images/virtual-addressing.png}
    \caption{Use of a page table}
\end{figure}

The processor stars by sending a virtual address to the MMU, which requests a page table entry address from memory. The page table entry is fetched from page table in memory, and the MMU sends the phydical address to cache or memory. Finally, the cache or memory sends the data word back to the processor. This is summarized in the following diagram:
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{images/MMU.png}
    \caption{Page hit}
\end{figure}

Translation can be sped up using a \emph{Translation Lookaside Buffer} (TLB). Page table entries (PTEs) are cached in L1 like any other memory word. PTE hit still requires a small L1 delay, and PTEs may be evicted by other data references. The solution to this is a \emph{Translation Lookaside Buffer} (TLB). It is a small hardware cache in the MMU, which maps virtual page numbers to physical page numbers. It contains complete page table entries for a small number of pages.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{images/TLB.png}
    \caption{Use of a Translation Lookaside Buffer (TLB)}
\end{figure}
The modified hit procedure using TLB is therefore of the form:
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/TLB-hit.png}
    \caption{TLB hit}
\end{figure}
Notice that a TLB hit eliminates a memory access. A TLB miss incus an additional memory acces (the PTE). Fortunately, TLB misses are rare.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/TLB-miss.png}
    \caption{TLB miss}
\end{figure}

\end{document}